\documentclass[10pt]{article}

\usepackage{epsfig}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{color}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage[svgname]{xcolor}

\topmargin -0.3in
\oddsidemargin 0in
\evensidemargin 0in
\headheight 12pt
\headsep 12pt
\textheight 8.85in
\textwidth 6.5in
\parindent 0pt

\raggedbottom

\def\beqa#1\eeqa{\begin{eqnarray}#1\end{eqnarray}}
\newcommand{\R}{{\cal{R}}}
\newcommand{\N}{{\cal{N}}}
\newcommand{\C}{{\cal{C}}}
\newcommand{\D}{{\cal{D}}}
\renewcommand{\S}{{\cal{S}}}
\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf{y}}}
\newcommand{\bX}{{\bf{X}}}
\newcommand{\bt}{{\bf{t}}}
\newcommand{\bw}{{\bf{w}}}
\newcommand{\bm}{{\bf{m}}}
\newcommand{\bv}{{\bf{v}}}
\newcommand{\bS}{{\bf{S}}}
\newcommand{\bM}{{\bf{M}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bPhi}{{\boldsymbol{\Phi}}}
\renewcommand{\d}{{\textrm{d}}}
\newcommand{\Figref}[1]{Fig.~\ref{#1}}
\newcommand{\Eqref}[1]{Eq.~\ref{#1}} % The '~' stops breaking and gives correct spacing
\newcommand{\Eqrefs}[2]{Eqs.~\ref{#1},~\ref{#2}}

\newcommand{\bh}{{\bf{h}}}
\newcommand{\ba}{{\bf{a}}}
\newcommand{\bb}{{\bf{b}}}
\newcommand{\Z}{{\cal{Z}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bSigma}{{\boldsymbol{\Sigma}}}

\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 2}}
\rhead{\fancyplain{}{10-707: Deep Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 2}} % Title

\newcommand{\outDate}{Oct 9, 2017}
\newcommand{\dueDate}{Oct 23, 2017 11:59 pm}

\author{CMU 10-707: \textsc{Deep Learning (Fall 2017)} \\
\url{https://piazza.com/cmu/fall2017/10707} \\
OUT: \outDate{} \\
DUE: \dueDate{} \\
TAs: Otilia Stretcu, Shunyuan Zhang}

\date{}
\begin{document}
\maketitle
\setcounter{page}{1}

%\pagestyle{myheadings}
%\setcounter{page}{1}

%\markboth{\hfil STA 4273H, Assignment 1}{
%          \hfil STA 4273H, Assignment 1}

%\vspace*{-32pt}


\section*{Problem 1 (10 pts)}



\section*{Problem 2 (10 pts)}

\beqa
p(\bx) = \prod_{k=1}^K p(x_k | \textrm{pa}_k) 
\eeqa


\section*{Problem 3 (10pts)}
\beqa
 E(\bv, \bh; \theta ) =
 -\sum_{i=1}^D \sum_{j=1}^P W_{ij} v_i h_j - \sum_{i=1}^D v_i b_i -
 \sum_{j=1}^P h_j a_j.
\eeqa

\beqa
 P_{\theta} (\bv,\bh) = \frac{1}{\Z} \exp(-E (\bv, \bh; \theta )),
\eeqa
\beqa
\Z = \sum_{\bv, \bh} \exp(-E (\bv, \bh; \theta ))
\eeqa
Proof Here
\beqa
 P_{\theta}(h_1,...,h_P | \bv) =
 \prod_{j=1}^P p_{\theta}(h_j | \bv),
\eeqa
where
\beqa
P_{\theta}(h_j = 1 | \bv) = \frac{1}{1 + \exp(-\sum_{i=1}^D W_{ij}v_i - a_j)}.
\eeqa

\section*{Problem 4 (10 pts)}
\beqa
E(\bx,\by) = h \sum_i x_i - \beta \sum_{i,j} x_i x_j - \eta \sum_i x_i y_i,
\eeqa
\beqa
p(\bx,\by) = \frac{1}{\Z} \exp{(-E(\bx,\by)}
\eeqa

\begin{itemize}
\item (5 pts)
Expression here
\item (5 pts)
Proof Here
\end{itemize}

\section*{Problem 5 (60 pts)}

For this question you will 
write your own implementation of the Contrastive Divergence algorithm 
for training RBMs, and compare it to an Autoencoder model. 
Please do not use any toolboxes. 
We recommend that you use MATLAB or Python, but you are welcome
to use any other programming language if you wish. 
\\
 
The goal is to build a generative model of images of
10 handwritten digits of ``zero'', ``one'',..., ``nine''.
The images are 28 by 28 in size (MNIST dataset), which we will be represented
as a vector $\bx$ of dimension 784 by listing all the pixel values in
raster scan order. The labels t are 0,1,2,...,9 corresponding to
10 classes as written in the image.
There are 3000 training cases, containing 300 examples of each of 10
classes,
1000 validation (100 examples of each of 10 classes), and 3000
test cases (300 examples of each of 10 classes).
they can be found in the file digitstrain.txt, digitsvalid.txt
 and digitstest.txt: \\

\url{http://www.cs.cmu.edu/~rsalakhu/10707/assignments.html}
\\
\\

{\bf Format of the data}: digitstrain.txt contains 3000 lines.
Each line contains 785 numbers (comma delimited):
the first 784 real-valued numbers
correspond to the
784 pixel values, and the last number denotes
the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc.
digitsvalid.txt and digitstest.txt contain 1000 and 3000 lines
and use the same format as above.


\subsubsection*{Contrastive Divergence (CD), Autoencoders}
Implement the CD algorithm for training an RBM model.
Implement both an autoencoder and a denoising autoencoder model. 


\subsubsection*{a) Basic generalization [20 points]}
Train an RBM model with 100 hidden units, starting with CD with $k = 1$ step. For initialization use samples from a normal distribution with mean 0 and standard deviation 0.1. Choose a reasonable 
learning rate (e.g. 0.1 or 0.01). Use your own judgement to decide on the stoping criteria (i.e number of epochs or convergence criteria). 
\\

As a proxy for monitoring the progress, 
plot the average training cross-entropy reconstruction error
on the y-axis vs. the epoch number (x-axis). 
On the same figure, plot the average validation cross-entropy error function.
\\
 
Examine the plots of training error and validation error.
How does the network's performance differ on the training set versus the validation set
during learning? 
Visualize the learned W as 100 28x28 images
(plot all filters as one image, as we have seen in class).
Do the learned features exhibit any structure? 

\subsubsection*{b) Number of CD steps [5 points]}
Try learning an RBM model with CD with $k = 5$, $k = 10$, and $k = 20$ steps.
Describe the effect of this modification on the convergence properties, and the
generalization of the network.


\subsubsection*{c) Sampling from the RBM model [5 points]}
To qualitatively test the model performance, initialize 100 Gibbs chains 
with random configurations of the visible variables, and run the Gibbs sampler 
for 1000 steps. Display the 100 sampled images. Do they look like handwritten digits?

\subsubsection*{d) Unsupervised Learning as pretraining [5 points]}
Use the weights you learned in question \textbf{a)} to initialize a 1-layer neural network with 100 hidden units. 
Train the resulting neural network to classify the 10 digits, as done in the first 
assignment. Does this pretraining help compared to training the same 
neural network initialized with random weights in terms of 
classification accuracy? Describe your findings.  

\subsubsection*{e) Autoencoder [5 points]}
Instead of training an RBM model, train an autoencoder model
with 100 sigmoid hidden units. For initializing the autoencoder, use samples from a normal distribution with mean 0 and standard deviation 0.1. 
Does the model learn useful filters? Visualize the learned weights.
Similar to RBMs, use the learned autoencoder 
weights to initialize a 1-layer neural network with 100 hidden units. 
Train the resulting neural network to classify the 10 digits.
How does it compare to pretraining with RBMs?
 

\subsubsection*{f) Denoising Autoencoder [10 points]}
Train a denosing autoencoder by using drop-out on the inputs. We recommend a drop-out rate of 10\%, but feel free to explore other rates.
Visualize the learned weights. Use the learned 
weights to initialize a 1-layer neural network with 100 hidden units.
Train the resulting neural network to classify the 10 digits.
How does it compare to pretrainig with standard autoencoders and RBMs
in terms of classification accuracy?


\subsubsection*{g) Number of hidden units [10 points]}
Try training RBMs, autoencoders and denoising autoencoders with 
50, 100, 200, and 500  hidden units.
Describe the effect of this modification on the convergence properties, and the
generalization of the network.








\end{document}
\grid
